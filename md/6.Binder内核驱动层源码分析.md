## Binder内核驱动层源码分析
在分析Media服务添加和ServiceManager启动时有会有一个操作那就是打开binder驱动
```c
//media服务
static int open_driver()
{
    int fd = open("/dev/binder", O_RDWR);
     ...
    return fd;
}

//servicemanager
struct binder_state *binder_open(size_t mapsize)
{
    // mapsize = 128k
    ...
    bs->fd = open("/dev/binder", O_RDWR);
    ...
}
```

发现都是通过低通调用，打开一个"/dev/binder"的文件（Linux系统会把程序也抽象成为一个文件）</br>
open函数的系统调用会判断，如果打开的时一个文件，则返回文件句柄，如果打开的是一个驱动，则进入驱动指定的系统函数名称</br></br>
使用的时安卓linux-3.18内核</br>
\linux-3.18\linux-3.18\drivers\staging\android\binder.c
```c
static int __init binder_init(void)
{
	int ret;
    
    // 创建binder的工作队列
	binder_deferred_workqueue = create_singlethread_workqueue("binder");
	if (!binder_deferred_workqueue)
		return -ENOMEM;
	  // 基于 misc_class 构造一个设备，将 miscdevice 结构挂载到 misc_list 列表上，并初始化与 linux 设备模型相关的结构 
	  //相当于这个驱动程序就时一个设备了
	ret = misc_register(&binder_miscdev);

	return ret;
}

static struct miscdevice binder_miscdev = {
    // 次设备号 动态分配
	.minor = MISC_DYNAMIC_MINOR,
	// 设备名称
	.name = "binder",
	// 设备的文件操作结构，这是 file_operations 结构   
	.fops = &binder_fops
};

// 相对应的一些操作
static const struct file_operations binder_fops = {
	.owner = THIS_MODULE,
	.poll = binder_poll,
	.unlocked_ioctl = binder_ioctl,
	.compat_ioctl = binder_ioctl,
	.mmap = binder_mmap,
	// 这里的open系统调用就调用到了驱动程序上的binder_open函数
	.open = binder_open,
	.flush = binder_flush,
	.release = binder_release,
};
```
binder_open的实现
```c
static int binder_open(struct inode *nodp, struct file *filp)
{
    // 当 binder 进程结构体
	struct binder_proc *proc;

    // 在内核区开辟连续空间，大小不能超过 128K，默认初始化值为 0 
	proc = kzalloc(sizeof(*proc), GFP_KERNEL);
	if (proc == NULL)
		return -ENOMEM;
	// 获取当前进程的task_struct
	get_task_struct(current);
	 // 将当前线程的 task 保存到 binder 进程的 tsk
	proc->tsk = current;
	 // 初始化 todo 列表
	INIT_LIST_HEAD(&proc->todo);
	// 初始化 wait 队列
	init_waitqueue_head(&proc->wait);
	// 将当前进程的 nice 值转换为进程优先级
	proc->default_priority = task_nice(current);
    // 同步锁，因为 binder 支持多线程访问
	binder_lock(__func__);
    // BINDER_PROC 对象创建数加1 
	binder_stats_created(BINDER_STAT_PROC);
	//将 proc_node 节点添加到 binder_procs 为表头的队列
	hlist_add_head(&proc->proc_node, &binder_procs);
	proc->pid = current->group_leader->pid;
	// 初始化已分发的死亡通知列表
	INIT_LIST_HEAD(&proc->delivered_death);
	//  file 文件指针的 private_data 变量指向 binder_proc 数据
	filp->private_data = proc;
    // 释放锁
	binder_unlock(__func__);
	...
	return 0;
}


struct binder_proc {
	struct hlist_node proc_node;
	// 线程树，处理客户端发起的请求
	struct rb_root threads;
	// nodes树 保存 binder_proc 进程内的 Binder 实体；
	struct rb_root nodes;
	//进程内的 Binder 引用，即引用的其它进程的 Binder 实体，以句柄作 key 值来组织
	struct rb_root refs_by_desc;
	// 进程内的 Binder 引用，即引用的其它进程的 Binder 实体，以地址作 key 值来组织
	struct rb_root refs_by_node;
	// 进程id
	int pid;
	
	struct vm_area_struct *vma;
	struct mm_struct *vma_vm_mm;
	// 当前线程的 task_struct
	struct task_struct *tsk;
	struct files_struct *files;
	struct hlist_node deferred_work_node;
	int deferred_work;
	// 指向内核虚拟内存的地址
	void *buffer;
	// 内核虚拟内存与用户空间地址偏移量
	ptrdiff_t user_buffer_offset;

	struct list_head buffers;
	struct rb_root free_buffers;
	struct rb_root allocated_buffers;
	size_t free_async_space;
    
    // 物理页的指针数组
    struct page **pages;
    // 映射虚拟内存的大小
	size_t buffer_size;
	uint32_t buffer_free;
	struct list_head todo;
	wait_queue_head_t wait;
	struct binder_stats stats;
	struct list_head delivered_death;
	int max_threads;
	int requested_threads;
	int requested_threads_started;
	int ready_threads;
	long default_priority;
	struct dentry *debugfs_entry;
};
```
创建 binder_proc 对象，并把当前进程等信息保存到 binder_proc 对象，该对象管理 IPC 所需的各种信息并拥有其他结构体的根结构体；再把 binder_proc 对象保存到文件指针 filp，以及把 binder_proc 加入到全局链表 binder_procs。


在ServiceManager中，打开binder驱动后会进行
```c
bs->mapped = mmap(NULL, mapsize, PROT_READ, MAP_PRIVATE, bs->fd, 0);
```
binder_mmap
```c
static int binder_mmap(struct file *filp, struct vm_area_struct *vma)
{
	int ret;
    // 内核虚拟空间
	struct vm_struct *area;
    // 从 filp 中获取之前打开保存的
	struct binder_proc *proc = filp->private_data;
	const char *failure_string;
	struct binder_buffer *buffer;

	if (proc->tsk != current)
		return -EINVAL;
    // 保证映射内存大小不超过 4M
	if ((vma->vm_end - vma->vm_start) > SZ_4M)
	    // 大于4M = 4M
		vma->vm_end = vma->vm_start + SZ_4M;

    ...
    
	mutex_lock(&binder_mmap_lock);
    //  采用 IOREMAP方式，分配一个连续的内核虚拟空间，与进程虚拟空间大小一致
	area = get_vm_area(vma->vm_end - vma->vm_start, VM_IOREMAP);
	
	// 指向内核虚拟空间的地址
	proc->buffer = area->addr;
	// 计算偏移量  根据内核地址能计算得到用户空间地址
	// 反过来，根据用户空间地址能找到内核的地址
	proc->user_buffer_offset = vma->vm_start - (uintptr_t)proc->buffer;
	mutex_unlock(&binder_mmap_lock);


    // 分配物理页的指针数组，数组大小为 vma 的等效 page 个数（4K一个，也就是一页4k的大小）；
	proc->pages = kzalloc(sizeof(proc->pages[0]) * ((vma->vm_end - vma->vm_start) / PAGE_SIZE), GFP_KERNEL);
	
	 // binder_buffer 对象 指向 proc 的 buffer 地址
	buffer = proc->buffer;
	// 将 binder_buffer 地址 加入到所属进程的 buffers 队列
	INIT_LIST_HEAD(&proc->buffers);
	list_add(&buffer->entry, &proc->buffers);
	buffer->free = 1;
	 // 将空闲 buffer 放入 proc->free_buffers 中
	binder_insert_free_buffer(proc, buffer);
	// 异步可用空间大小为 buffer 总大小的一半。
	proc->free_async_space = proc->buffer_size / 2;
	barrier();
	proc->files = get_files_struct(current);
	proc->vma = vma;
	proc->vma_vm_mm = vma->vm_mm;
	
    return ret;
}

```
binder_mmap主要就是通过内核空间将用户空间与屋里内存形成一个映射关系</br>
task_struct：代表的是进程或是线程管理控制的一个结构体</br>
mm_struct ： task_struct 结构体中虚拟地址管理的结构体</br>
vm_area_struct： 代表的是虚拟用户空间映射管理的结构体</br>
vm_struct：代表的是内核空间管理的结构体</br>
alloc_page： 方法的作用是分配一个物理内存</br>
map_kernel_range_noflush： 方法的作用是将物理空间映射到虚拟内核空间</br>
vm_insert_page： 方法的作用是将物理空间映射到虚拟用户空间</br>
binder_mmap 的主要作用就是开辟一块连续的内核空间，并且开辟一个物理页的地址空间，同时映射到用户空间和内核空间</br></br>
![binder_mmp](https://github.com/TF27674569/Other-Data/blob/master/image/binder_mmap.png)


先看一下结构图</br>
![包装的结构](https://github.com/TF27674569/Other-Data/blob/master/image/Binder_Data.png)


binder_ioctl
```c
static long binder_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
{
	int ret;
	struct binder_proc *proc = filp->private_data;
	struct binder_thread *thread; // binder线程
	unsigned int size = _IOC_SIZE(cmd);
	void __user *ubuf = (void __user *)arg;

	trace_binder_ioctl(cmd, arg);
    // 第一次不会等待
	ret = wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);
	if (ret)
		goto err_unlocked;

	binder_lock(__func__);
	// 去红黑树里面拿thread
	thread = binder_get_thread(proc);
	if (thread == NULL) {
		ret = -ENOMEM;
		goto err;
	}
    
    // 根据指令调用
	switch (cmd) {
	case BINDER_WRITE_READ:
	    // 因为要数据
	    // 最后调用的是这个（BINDER_WRITE_READ）
		ret = binder_ioctl_write_read(filp, cmd, arg, thread);
		if (ret)
			goto err;
		break;
	// 设置 binder 最大支持的线程数
	case BINDER_SET_MAX_THREADS:
		if (copy_from_user(&proc->max_threads, ubuf, sizeof(proc->max_threads))) {
			ret = -EINVAL;
			goto err;
		}
		break;
	// 成为binder上下文管理者（ServiceManager）
	// ioctl(bs->fd, BINDER_SET_CONTEXT_MGR, 0);
	case BINDER_SET_CONTEXT_MGR:
		ret = binder_ioctl_set_ctx_mgr(filp);
		if (ret)
			goto err;
		break;
	// 当 binder 线程退出，释放 binder 线程
	case BINDER_THREAD_EXIT:
		binder_debug(BINDER_DEBUG_THREADS, "%d:%d exit\n",
			     proc->pid, thread->pid);
		binder_free_thread(proc, thread);
		thread = NULL;
		break;
	// binder版本
	case BINDER_VERSION: {
		struct binder_version __user *ver = ubuf;

		if (size != sizeof(struct binder_version)) {
			ret = -EINVAL;
			goto err;
		}
		if (put_user(BINDER_CURRENT_PROTOCOL_VERSION,
			     &ver->protocol_version)) {
			ret = -EINVAL;
			goto err;
		}
		break;
	}
	default:
		ret = -EINVAL;
		goto err;
	}
	ret = 0;
err:
	if (thread)
		thread->looper &= ~BINDER_LOOPER_STATE_NEED_RETURN;
	binder_unlock(__func__);
	wait_event_interruptible(binder_user_error_wait, binder_stop_on_user_error < 2);
	if (ret && ret != -ERESTARTSYS)
		pr_info("%d:%d ioctl %x %lx returned %d\n", proc->pid, current->pid, cmd, arg, ret);
err_unlocked:
	trace_binder_ioctl_done(ret);
	return ret;
}

```
binder_get_thread红黑树里面获取数据
```c
static struct binder_thread *binder_get_thread(struct binder_proc *proc)
{
	struct binder_thread *thread = NULL;
	struct rb_node *parent = NULL;
	// binder_proc的节点
	struct rb_node **p = &proc->threads.rb_node;

	while (*p) {
		parent = *p;
		thread = rb_entry(parent, struct binder_thread, rb_node);

		if (current->pid < thread->pid)
			p = &(*p)->rb_left;
		else if (current->pid > thread->pid)
			p = &(*p)->rb_right;
		else
			break;
	}
	
	// 第一次为null 直接创建
	if (*p == NULL) {
		thread = kzalloc(sizeof(*thread), GFP_KERNEL);
		if (thread == NULL)
			return NULL;
		binder_stats_created(BINDER_STAT_THREAD);
		thread->proc = proc;
		// 线程id为进程的id
		thread->pid = current->pid;
		init_waitqueue_head(&thread->wait);
		// 初始化线程的 todo 队列
		INIT_LIST_HEAD(&thread->todo);
		//  把线程加入 proc->threads 
		rb_link_node(&thread->rb_node, parent, p);
		rb_insert_color(&thread->rb_node, &proc->threads);
		thread->looper |= BINDER_LOOPER_STATE_NEED_RETURN;
		thread->return_error = BR_OK;
		thread->return_error2 = BR_OK;
	}
	return thread;
}

```
![thread_tree](https://github.com/TF27674569/Other-Data/blob/master/image/thread_tree.png)



最外层的指令是BINDER_WRITE_READ函数(binder_ioctl_write_read)
```c
static int binder_ioctl_write_read(struct file *filp,
				unsigned int cmd, unsigned long arg,
				struct binder_thread *thread)
{
	int ret = 0;
	struct binder_proc *proc = filp->private_data;
	unsigned int size = _IOC_SIZE(cmd);
	void __user *ubuf = (void __user *)arg;
	struct binder_write_read bwr;

	if (size != sizeof(struct binder_write_read)) {
		ret = -EINVAL;
		goto out;
	}
	
	// 拷贝一份数据
	if (copy_from_user(&bwr, ubuf, sizeof(bwr))) {
		ret = -EFAULT;
		goto out;
	}
	
    ...	
    // 当写缓存中有写数据，则执行 binder 写操作
	if (bwr.write_size > 0) {
		ret = binder_thread_write(proc, thread,
					  bwr.write_buffer,
					  bwr.write_size,
					  &bwr.write_consumed);
		trace_binder_write_done(ret);
		if (ret < 0) {
		    // 当写失败，再将 bwr 数据写回用户空间，并返回
			bwr.read_consumed = 0;
			if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
				ret = -EFAULT;
			goto out;
		}
	}
	
	 // 当读缓存中有读数据，则执行 binder 读操作
	if (bwr.read_size > 0) {
		ret = binder_thread_read(proc, thread, bwr.read_buffer,
					 bwr.read_size,
					 &bwr.read_consumed,
					 filp->f_flags & O_NONBLOCK);
		trace_binder_read_done(ret);
		if (!list_empty(&proc->todo))
		     // 唤醒等待状态的线程
			wake_up_interruptible(&proc->wait);
			
		 // 当读失败，再将bwr数据写回用户空间，并返回
		if (ret < 0) {
			if (copy_to_user(ubuf, &bwr, sizeof(bwr)))
				ret = -EFAULT;
			goto out;
		}
	}
	binder_debug(BINDER_DEBUG_READ_WRITE,
		     "%d:%d wrote %lld of %lld, read return %lld of %lld\n",
		     proc->pid, thread->pid,
		     (u64)bwr.write_consumed, (u64)bwr.write_size,
		     (u64)bwr.read_consumed, (u64)bwr.read_size);
		     
    // 将内核数据 bwr 拷贝到用户空间 ubuf
	if (copy_to_user(ubuf, &bwr, sizeof(bwr))) {
		ret = -EFAULT;
		goto out;
	}
out:
	return ret;
}
```


binder_thread_write</br>
binder_thread_read</br>
这两个比较复杂
